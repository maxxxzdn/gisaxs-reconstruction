Loading module cuda/11.2 
Loading module anaconda/2021.11 
Loading module gcc/7.3.0 
/home/zhdano82/.conda/envs/pyg/lib/python3.8/site-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: /home/zhdano82/.conda/envs/pyg/lib/python3.8/site-packages/torchvision/image.so: undefined symbol: _ZN5torch3jit17parseSchemaOrNameERKNSt7__cxx1112basic_stringIcSt11char_traitsIcESaIcEEE
  warn(f"Failed to load image Python extension: {e}")
Multiprocessing is handled by SLURM.
GPU available: True, used: True
TPU available: False, using: 0 TPU cores
IPU available: False, using: 0 IPUs
HPU available: False, using: 0 HPUs
/home/zhdano82/SimBasedInference/gisaxs-reconstruction/scripts/../dataset/ba_dataset.py:69: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  self.X = np.array(self.X)[np.where(nons)[0]]
/home/zhdano82/SimBasedInference/gisaxs-reconstruction/scripts/../dataset/ba_dataset.py:69: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  self.X = np.array(self.X)[np.where(nons)[0]]
/home/zhdano82/SimBasedInference/gisaxs-reconstruction/scripts/../dataset/ba_dataset.py:70: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  self.y = np.array(self.y)[np.where(nons)[0]]
/home/zhdano82/SimBasedInference/gisaxs-reconstruction/scripts/../dataset/ba_dataset.py:70: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  self.y = np.array(self.y)[np.where(nons)[0]]
/home/zhdano82/SimBasedInference/gisaxs-reconstruction/scripts/../dataset/ba_dataset.py:71: FutureWarning: The input object of type 'Tensor' is an array-like implementing one of the corresponding protocols (`__array__`, `__array_interface__` or `__array_struct__`); but not a sequence (or 0-D). In the future, this object will be coerced as if it was first converted using `np.array(obj)`. To retain the old behaviour, you have to either modify the type 'Tensor', or assign to an empty array created with `np.empty(correct_shape, dtype=object)`.
  self.x = np.array(self.x)[np.where(nons)[0]]
/home/zhdano82/SimBasedInference/gisaxs-reconstruction/scripts/../dataset/ba_dataset.py:71: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray.
  self.x = np.array(self.x)[np.where(nons)[0]]
LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]

  | Name | Type | Params
------------------------------
0 | cvae | cVAE | 338 K 
1 | flow | Flow | 408 K 
------------------------------
747 K     Trainable params
0         Non-trainable params
747 K     Total params
2.989     Total estimated model params size (MB)
SLURM auto-requeueing enabled. Setting signal handlers.
Sanity Checking: 0it [00:00, ?it/s]Sanity Checking DataLoader 0:   0%|          | 0/2 [00:00<?, ?it/s]Sanity Checking DataLoader 0: 100%|██████████| 2/2 [00:00<00:00,  8.19it/s]                                                                           /home/zhdano82/.conda/envs/pyg/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, val_dataloader 0, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
/home/zhdano82/.conda/envs/pyg/lib/python3.8/site-packages/pytorch_lightning/trainer/connectors/data_connector.py:240: PossibleUserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 64 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.
  rank_zero_warn(
Training: 0it [00:00, ?it/s]Epoch 0:   0%|          | 0/16935 [00:00<?, ?it/s]Epoch 0:   6%|▌         | 1000/16935 [00:47<12:29, 21.27it/s]Epoch 0:   6%|▌         | 1000/16935 [00:47<12:29, 21.27it/s, loss=-314, kl=7.770, nf_loss=8.560, elbo=-349., lp=-9.55]Epoch 0:  12%|█▏        | 2000/16935 [01:33<11:41, 21.28it/s, loss=-314, kl=7.770, nf_loss=8.560, elbo=-349., lp=-9.55]Epoch 0:  12%|█▏        | 2000/16935 [01:33<11:41, 21.28it/s, loss=-1.86e+03, kl=9.230, nf_loss=7.690, elbo=-1.86e+3, lp=-8.93]Epoch 0:  18%|█▊        | 3000/16935 [02:20<10:54, 21.29it/s, loss=-1.86e+03, kl=9.230, nf_loss=7.690, elbo=-1.86e+3, lp=-8.93]Epoch 0:  18%|█▊        | 3000/16935 [02:20<10:54, 21.29it/s, loss=-2.74e+03, kl=11.20, nf_loss=6.110, elbo=-2.79e+3, lp=-7.28]Epoch 0:  24%|██▎       | 4000/16935 [03:07<10:07, 21.29it/s, loss=-2.74e+03, kl=11.20, nf_loss=6.110, elbo=-2.79e+3, lp=-7.28]Epoch 0:  24%|██▎       | 4000/16935 [03:07<10:07, 21.29it/s, loss=-2.98e+03, kl=12.60, nf_loss=6.050, elbo=-3.02e+3, lp=-7.30]Epoch 0:  30%|██▉       | 5000/16935 [03:54<09:20, 21.28it/s, loss=-2.98e+03, kl=12.60, nf_loss=6.050, elbo=-3.02e+3, lp=-7.30]Epoch 0:  30%|██▉       | 5000/16935 [03:54<09:20, 21.28it/s, loss=-3.03e+03, kl=12.30, nf_loss=6.190, elbo=-2.82e+3, lp=-7.70]Epoch 0:  35%|███▌      | 6000/16935 [04:41<08:33, 21.29it/s, loss=-3.03e+03, kl=12.30, nf_loss=6.190, elbo=-2.82e+3, lp=-7.70]Epoch 0:  35%|███▌      | 6000/16935 [04:41<08:33, 21.29it/s, loss=-3.07e+03, kl=12.90, nf_loss=5.100, elbo=-2.94e+3, lp=-6.48]Epoch 0:  41%|████▏     | 7000/16935 [05:28<07:46, 21.29it/s, loss=-3.07e+03, kl=12.90, nf_loss=5.100, elbo=-2.94e+3, lp=-6.48]Epoch 0:  41%|████▏     | 7000/16935 [05:28<07:46, 21.29it/s, loss=-3.15e+03, kl=11.90, nf_loss=6.890, elbo=-3.25e+3, lp=-8.42]Epoch 0:  47%|████▋     | 8000/16935 [06:15<06:59, 21.28it/s, loss=-3.15e+03, kl=11.90, nf_loss=6.890, elbo=-3.25e+3, lp=-8.42]Epoch 0:  47%|████▋     | 8000/16935 [06:15<06:59, 21.28it/s, loss=-3.14e+03, kl=12.50, nf_loss=4.710, elbo=-3.38e+3, lp=-6.36]Epoch 0:  53%|█████▎    | 9000/16935 [07:06<06:15, 21.11it/s, loss=-3.14e+03, kl=12.50, nf_loss=4.710, elbo=-3.38e+3, lp=-6.36]Epoch 0:  53%|█████▎    | 9000/16935 [07:06<06:15, 21.11it/s, loss=-3.18e+03, kl=12.60, nf_loss=5.700, elbo=-3.29e+3, lp=-7.02]Epoch 0:  59%|█████▉    | 10000/16935 [07:53<05:28, 21.12it/s, loss=-3.18e+03, kl=12.60, nf_loss=5.700, elbo=-3.29e+3, lp=-7.02]Epoch 0:  59%|█████▉    | 10000/16935 [07:53<05:28, 21.12it/s, loss=-3.16e+03, kl=12.10, nf_loss=4.520, elbo=-3.35e+3, lp=-6.16]Epoch 0:  65%|██████▍   | 11000/16935 [08:40<04:40, 21.13it/s, loss=-3.16e+03, kl=12.10, nf_loss=4.520, elbo=-3.35e+3, lp=-6.16]Epoch 0:  65%|██████▍   | 11000/16935 [08:40<04:40, 21.13it/s, loss=-3.1e+03, kl=13.00, nf_loss=3.820, elbo=-3.1e+3, lp=-5.83]  Epoch 0:  71%|███████   | 12000/16935 [09:27<03:53, 21.15it/s, loss=-3.1e+03, kl=13.00, nf_loss=3.820, elbo=-3.1e+3, lp=-5.83]Epoch 0:  71%|███████   | 12000/16935 [09:27<03:53, 21.15it/s, loss=-3.14e+03, kl=12.00, nf_loss=5.240, elbo=-3.11e+3, lp=-6.49]Epoch 0:  77%|███████▋  | 13000/16935 [10:14<03:05, 21.16it/s, loss=-3.14e+03, kl=12.00, nf_loss=5.240, elbo=-3.11e+3, lp=-6.49]Epoch 0:  77%|███████▋  | 13000/16935 [10:14<03:05, 21.16it/s, loss=-3.17e+03, kl=13.20, nf_loss=4.080, elbo=-2.85e+3, lp=-5.68]Epoch 0:  80%|████████  | 13548/16935 [10:40<02:40, 21.16it/s, loss=-3.17e+03, kl=13.20, nf_loss=4.080, elbo=-2.85e+3, lp=-5.68]Epoch 0:  80%|████████  | 13548/16935 [10:40<02:40, 21.16it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-2.94e+3, lp=-6.83]
Validation: 0it [00:00, ?it/s][A
Validation DataLoader 0:   0%|          | 0/3387 [00:00<?, ?it/s][A
Validation DataLoader 0:  30%|██▉       | 1000/3387 [00:14<00:33, 70.54it/s][AEpoch 0:  86%|████████▌ | 14548/16935 [10:54<01:47, 22.23it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-2.94e+3, lp=-6.83]
Validation DataLoader 0:  59%|█████▉    | 2000/3387 [00:28<00:19, 70.74it/s][AEpoch 0:  92%|█████████▏| 15548/16935 [11:08<00:59, 23.26it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-2.94e+3, lp=-6.83]
Validation DataLoader 0:  89%|████████▊ | 3000/3387 [00:42<00:05, 70.78it/s][AEpoch 0:  98%|█████████▊| 16548/16935 [11:22<00:15, 24.24it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-2.94e+3, lp=-6.83]
Validation DataLoader 0: 100%|██████████| 3387/3387 [00:47<00:00, 70.79it/s][AEpoch 0: 100%|██████████| 16935/16935 [11:28<00:00, 24.61it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-2.94e+3, lp=-6.83]Epoch 0: 100%|██████████| 16935/16935 [11:28<00:00, 24.61it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-3.47e+3, lp=-6.83, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]
                                                                            [AEpoch 0: 100%|██████████| 16935/16935 [11:28<00:00, 24.61it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-3.47e+3, lp=-6.83, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1: 100%|██████████| 16935/16935 [11:28<00:00, 24.61it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-3.47e+3, lp=-6.83, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:   6%|▌         | 1000/16935 [12:15<3:15:13,  1.36it/s, loss=-3.19e+03, kl=12.40, nf_loss=5.750, elbo=-3.47e+3, lp=-6.83, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:   6%|▌         | 1000/16935 [12:15<3:15:14,  1.36it/s, loss=-3.15e+03, kl=12.80, nf_loss=3.550, elbo=-3.24e+3, lp=-5.30, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  12%|█▏        | 2000/16935 [13:02<1:37:20,  2.56it/s, loss=-3.15e+03, kl=12.80, nf_loss=3.550, elbo=-3.24e+3, lp=-5.30, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  12%|█▏        | 2000/16935 [13:02<1:37:20,  2.56it/s, loss=-3.16e+03, kl=12.80, nf_loss=4.230, elbo=-3.03e+3, lp=-5.54, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  18%|█▊        | 3000/16935 [13:49<1:04:11,  3.62it/s, loss=-3.16e+03, kl=12.80, nf_loss=4.230, elbo=-3.03e+3, lp=-5.54, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  18%|█▊        | 3000/16935 [13:49<1:04:11,  3.62it/s, loss=-3.24e+03, kl=12.50, nf_loss=3.540, elbo=-3.27e+3, lp=-5.07, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  24%|██▎       | 4000/16935 [14:36<47:13,  4.57it/s, loss=-3.24e+03, kl=12.50, nf_loss=3.540, elbo=-3.27e+3, lp=-5.07, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]  Epoch 1:  24%|██▎       | 4000/16935 [14:36<47:13,  4.57it/s, loss=-3.2e+03, kl=12.90, nf_loss=3.430, elbo=-3.27e+3, lp=-5.23, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97] Epoch 1:  30%|██▉       | 5000/16935 [15:23<36:43,  5.42it/s, loss=-3.2e+03, kl=12.90, nf_loss=3.430, elbo=-3.27e+3, lp=-5.23, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  30%|██▉       | 5000/16935 [15:23<36:43,  5.42it/s, loss=-3.19e+03, kl=12.70, nf_loss=4.360, elbo=-3e+3, lp=-6.50, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]  Epoch 1:  35%|███▌      | 6000/16935 [16:10<29:28,  6.18it/s, loss=-3.19e+03, kl=12.70, nf_loss=4.360, elbo=-3e+3, lp=-6.50, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  35%|███▌      | 6000/16935 [16:10<29:28,  6.18it/s, loss=-3.2e+03, kl=13.50, nf_loss=2.270, elbo=-3.06e+3, lp=-4.17, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  41%|████▏     | 7000/16935 [16:57<24:03,  6.88it/s, loss=-3.2e+03, kl=13.50, nf_loss=2.270, elbo=-3.06e+3, lp=-4.17, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  41%|████▏     | 7000/16935 [16:57<24:03,  6.88it/s, loss=-3.26e+03, kl=13.10, nf_loss=3.530, elbo=-3.31e+3, lp=-5.62, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  47%|████▋     | 8000/16935 [17:44<19:48,  7.52it/s, loss=-3.26e+03, kl=13.10, nf_loss=3.530, elbo=-3.31e+3, lp=-5.62, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  47%|████▋     | 8000/16935 [17:44<19:48,  7.52it/s, loss=-3.23e+03, kl=12.50, nf_loss=3.220, elbo=-3.34e+3, lp=-5.27, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  53%|█████▎    | 9000/16935 [18:31<16:19,  8.10it/s, loss=-3.23e+03, kl=12.50, nf_loss=3.220, elbo=-3.34e+3, lp=-5.27, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  53%|█████▎    | 9000/16935 [18:31<16:19,  8.10it/s, loss=-3.27e+03, kl=13.50, nf_loss=4.250, elbo=-3.45e+3, lp=-5.97, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  59%|█████▉    | 10000/16935 [19:18<13:23,  8.63it/s, loss=-3.27e+03, kl=13.50, nf_loss=4.250, elbo=-3.45e+3, lp=-5.97, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  59%|█████▉    | 10000/16935 [19:18<13:23,  8.63it/s, loss=-3.25e+03, kl=12.50, nf_loss=3.830, elbo=-3.35e+3, lp=-5.84, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  65%|██████▍   | 11000/16935 [20:05<10:50,  9.13it/s, loss=-3.25e+03, kl=12.50, nf_loss=3.830, elbo=-3.35e+3, lp=-5.84, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  65%|██████▍   | 11000/16935 [20:05<10:50,  9.13it/s, loss=-3.18e+03, kl=13.00, nf_loss=2.270, elbo=-3.18e+3, lp=-4.72, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  71%|███████   | 12000/16935 [20:52<08:35,  9.58it/s, loss=-3.18e+03, kl=13.00, nf_loss=2.270, elbo=-3.18e+3, lp=-4.72, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  71%|███████   | 12000/16935 [20:52<08:35,  9.58it/s, loss=-3.22e+03, kl=13.40, nf_loss=4.140, elbo=-3.21e+3, lp=-5.45, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  77%|███████▋  | 13000/16935 [21:39<06:33, 10.00it/s, loss=-3.22e+03, kl=13.40, nf_loss=4.140, elbo=-3.21e+3, lp=-5.45, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  77%|███████▋  | 13000/16935 [21:39<06:33, 10.00it/s, loss=-3.23e+03, kl=12.70, nf_loss=1.440, elbo=-2.96e+3, lp=-3.56, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  80%|████████  | 13548/16935 [22:05<05:31, 10.22it/s, loss=-3.23e+03, kl=12.70, nf_loss=1.440, elbo=-2.96e+3, lp=-3.56, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1:  80%|████████  | 13548/16935 [22:05<05:31, 10.22it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.03e+3, lp=-5.71, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]
Validation: 0it [00:00, ?it/s][A
Validation DataLoader 0:   0%|          | 0/3387 [00:00<?, ?it/s][A
Validation DataLoader 0:  30%|██▉       | 1000/3387 [00:14<00:33, 70.65it/s][AEpoch 1:  86%|████████▌ | 14548/16935 [22:19<03:39, 10.86it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.03e+3, lp=-5.71, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]
Validation DataLoader 0:  59%|█████▉    | 2000/3387 [00:28<00:19, 70.83it/s][AEpoch 1:  92%|█████████▏| 15548/16935 [22:33<02:00, 11.49it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.03e+3, lp=-5.71, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]
Validation DataLoader 0:  89%|████████▊ | 3000/3387 [00:42<00:05, 70.83it/s][AEpoch 1:  98%|█████████▊| 16548/16935 [22:47<00:31, 12.10it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.03e+3, lp=-5.71, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]
Validation DataLoader 0: 100%|██████████| 3387/3387 [00:47<00:00, 70.84it/s][AEpoch 1: 100%|██████████| 16935/16935 [22:52<00:00, 12.34it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.03e+3, lp=-5.71, val_kl=12.70, val_loss=-3.47e+3, val_nf_loss=4.350, val_lp=-4.97]Epoch 1: 100%|██████████| 16935/16935 [22:52<00:00, 12.34it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.44e+3, lp=-5.71, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]
                                                                            [AEpoch 1: 100%|██████████| 16935/16935 [22:52<00:00, 12.34it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.44e+3, lp=-5.71, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2: 100%|██████████| 16935/16935 [22:52<00:00, 12.34it/s, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.44e+3, lp=-5.71, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:   6%|▌         | 1000/16935 [23:39<6:17:05,  1.42s/it, loss=-3.25e+03, kl=13.00, nf_loss=4.490, elbo=-3.44e+3, lp=-5.71, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:   6%|▌         | 1000/16935 [23:39<6:17:05,  1.42s/it, loss=-3.2e+03, kl=12.90, nf_loss=2.310, elbo=-3.31e+3, lp=-4.44, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66] Epoch 2:  12%|█▏        | 2000/16935 [24:26<3:02:33,  1.36it/s, loss=-3.2e+03, kl=12.90, nf_loss=2.310, elbo=-3.31e+3, lp=-4.44, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  12%|█▏        | 2000/16935 [24:26<3:02:33,  1.36it/s, loss=-3.22e+03, kl=13.20, nf_loss=3.130, elbo=-3.01e+3, lp=-4.68, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  18%|█▊        | 3000/16935 [25:13<1:57:11,  1.98it/s, loss=-3.22e+03, kl=13.20, nf_loss=3.130, elbo=-3.01e+3, lp=-4.68, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  18%|█▊        | 3000/16935 [25:13<1:57:11,  1.98it/s, loss=-3.28e+03, kl=13.40, nf_loss=2.450, elbo=-3.34e+3, lp=-4.32, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  24%|██▎       | 4000/16935 [26:00<1:24:07,  2.56it/s, loss=-3.28e+03, kl=13.40, nf_loss=2.450, elbo=-3.34e+3, lp=-4.32, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  24%|██▎       | 4000/16935 [26:00<1:24:07,  2.56it/s, loss=-3.25e+03, kl=13.20, nf_loss=2.220, elbo=-3.33e+3, lp=-4.57, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  30%|██▉       | 5000/16935 [26:47<1:03:57,  3.11it/s, loss=-3.25e+03, kl=13.20, nf_loss=2.220, elbo=-3.33e+3, lp=-4.57, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  30%|██▉       | 5000/16935 [26:47<1:03:57,  3.11it/s, loss=-3.23e+03, kl=13.30, nf_loss=2.650, elbo=-2.98e+3, lp=-5.10, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  35%|███▌      | 6000/16935 [27:37<50:21,  3.62it/s, loss=-3.23e+03, kl=13.30, nf_loss=2.650, elbo=-2.98e+3, lp=-5.10, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]  Epoch 2:  35%|███▌      | 6000/16935 [27:37<50:21,  3.62it/s, loss=-3.26e+03, kl=12.80, nf_loss=1.820, elbo=-3.11e+3, lp=-3.65, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  41%|████▏     | 7000/16935 [28:36<40:35,  4.08it/s, loss=-3.26e+03, kl=12.80, nf_loss=1.820, elbo=-3.11e+3, lp=-3.65, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  41%|████▏     | 7000/16935 [28:36<40:35,  4.08it/s, loss=-3.3e+03, kl=12.60, nf_loss=2.920, elbo=-3.4e+3, lp=-5.28, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]  Epoch 2:  47%|████▋     | 8000/16935 [29:29<32:56,  4.52it/s, loss=-3.3e+03, kl=12.60, nf_loss=2.920, elbo=-3.4e+3, lp=-5.28, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  47%|████▋     | 8000/16935 [29:29<32:56,  4.52it/s, loss=-3.27e+03, kl=12.90, nf_loss=2.170, elbo=-3.43e+3, lp=-4.12, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  53%|█████▎    | 9000/16935 [30:16<26:41,  4.95it/s, loss=-3.27e+03, kl=12.90, nf_loss=2.170, elbo=-3.43e+3, lp=-4.12, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  53%|█████▎    | 9000/16935 [30:16<26:41,  4.95it/s, loss=-3.31e+03, kl=13.00, nf_loss=3.110, elbo=-3.53e+3, lp=-5.03, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  59%|█████▉    | 10000/16935 [31:03<21:32,  5.37it/s, loss=-3.31e+03, kl=13.00, nf_loss=3.110, elbo=-3.53e+3, lp=-5.03, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  59%|█████▉    | 10000/16935 [31:03<21:32,  5.37it/s, loss=-3.29e+03, kl=12.40, nf_loss=2.660, elbo=-3.46e+3, lp=-4.90, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  65%|██████▍   | 11000/16935 [31:50<17:10,  5.76it/s, loss=-3.29e+03, kl=12.40, nf_loss=2.660, elbo=-3.46e+3, lp=-4.90, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  65%|██████▍   | 11000/16935 [31:50<17:10,  5.76it/s, loss=-3.21e+03, kl=12.30, nf_loss=1.960, elbo=-3.17e+3, lp=-4.86, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  71%|███████   | 12000/16935 [32:37<13:25,  6.13it/s, loss=-3.21e+03, kl=12.30, nf_loss=1.960, elbo=-3.17e+3, lp=-4.86, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  71%|███████   | 12000/16935 [32:37<13:25,  6.13it/s, loss=-3.26e+03, kl=12.90, nf_loss=3.060, elbo=-3.24e+3, lp=-4.55, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  77%|███████▋  | 13000/16935 [33:24<10:06,  6.48it/s, loss=-3.26e+03, kl=12.90, nf_loss=3.060, elbo=-3.24e+3, lp=-4.55, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]Epoch 2:  77%|███████▋  | 13000/16935 [33:24<10:06,  6.48it/s, loss=-3.25e+03, kl=13.30, nf_loss=0.767, elbo=-2.95e+3, lp=-3.25, val_kl=13.60, val_loss=-3.44e+3, val_nf_loss=2.900, val_lp=-3.66]slurmstepd: error: *** JOB 5717530 ON gv026 CANCELLED AT 2023-03-06T09:56:48 ***
